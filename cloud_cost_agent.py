"""
Main script that loads synthetic data (generated by data_generator.py),
defines local tools, constructs the LlmAgent and InMemoryRunner, and runs a sample analysis.
"""
import asyncio
import json
import os
import time
from pathlib import Path
from typing import List

import pandas as pd
from google.adk.tools import AgentTool

os.environ["GOOGLE_API_KEY"] = ""

# imports for agent
from google.adk.agents import LlmAgent
from google.adk.models.google_llm import Gemini
from google.adk.runners import InMemoryRunner
from google.genai import types

# import data generation helpers
import data_generator as dg

# Make defaults
DATA_DIR = Path("data")
BILLING_CSV = DATA_DIR / "synthetic_billing.csv"
METRICS_JSONL = DATA_DIR / "synthetic_metrics.jsonl"
ASSETS_JSON = DATA_DIR / "assets.json"

# Local in-memory caches (loaded soon)
billing_df: pd.DataFrame = pd.DataFrame()
metrics_list: List[dict] = []
assets_list: List[dict] = []

# --- Utility: ensure data exists (load or generate) ---
def load_or_generate_data(generate_if_missing: bool = True):
    """
    Loads data from ./data; if any file is missing and generate_if_missing True,
    generate all files with data_generator.generate_all().
    """
    global billing_df, metrics_list, assets_list

    missing = []
    for path in (BILLING_CSV, METRICS_JSONL, ASSETS_JSON):
        if not path.exists():
            missing.append(path)

    if missing:
        if generate_if_missing:
            print("Missing data files found — generating synthetic data...")
            dg.generate_all(out_dir=str(DATA_DIR), days=365, projects=30)
        else:
            raise FileNotFoundError(f"Missing data files: {missing}")

    # load billing as DataFrame
    billing_df = pd.read_csv(BILLING_CSV, parse_dates=["usage_start_time"])
    # load metrics as list of dicts
    with open(METRICS_JSONL, "r") as f:
        metrics_list = [json.loads(l) for l in f]
    # assets
    with open(ASSETS_JSON, "r") as f:
        assets_list = json.load(f)

    print("Loaded data:")
    print(f" - billing rows: {len(billing_df)}")
    print(f" - metrics lines: {len(metrics_list)}")
    print(f" - assets: {len(assets_list)}")


# --- Local tool functions (to be passed to the agent) ---
def bq_query_cost_by_project(project_id: str, num_days: int):
    cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=num_days)
    df = billing_df[billing_df["usage_start_time"] >= cutoff.strftime("%Y-%m-%d")]
    out = df.groupby("project_id").cost.sum().reset_index().to_dict(orient="records")
    return out


def monitoring_fetch_cpu(days: int):
    # return up to last `days` entries
    return metrics_list[-days:]


def ticket_create(title: str, body: str):
    ticket = {
        "ticket_id": f"TCK-{abs(hash(title)) % 100000}",
        "title": title,
        "body": body,
        "created_at": time.time()
    }
    return ticket


# --- Build the agent and runner ---
def build_cloud_cost_agent():
    retry_config = types.HttpRetryOptions(
        attempts=5,  # Maximum retry attempts
        exp_base=7,  # Delay multiplier
        initial_delay=1,
        http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors
    )

    # model choice — keep same as your original sample
    model = Gemini(model="gemini-2.5-flash-lite", retry_options=retry_config)

    spike_detector_agent = LlmAgent(
        name="spike_detector_agent",
        model=model,
        instruction=(
            "You are a Spike Detector agent. INPUT: JSON array named 'rows' containing objects "
            "with at least 'usage_start_time' and 'cost'.\n\n"
            "TASK: Return a single JSON object ONLY (no prose) with keys:\n"
            "  {\"spikes\": [<rows that are spikes>], \"reason\": \"short explanation\"}\n\n"
            "Constraints: Output MUST be valid JSON. The 'spikes' list should contain the original "
            "row objects (or objects with usage_start_time and cost). If none, return spikes: [] and reason: 'none'."
        )
    )

    cloud_cost_agent = LlmAgent(
        name="cloud_cost_agent",
        model=model,
        instruction=(
            "You are a Cloud Cost Agent. You will be given project billing rows and recent metrics.\n"
            "When you need to determine whether there are cost spikes, CALL the `spike_detector_tool` "
            "with a JSON payload: {\"rows\": [ ... ]}. The spike detector will return JSON: "
            "{\"spikes\": [...], \"reason\": \"...\"}.\n\n"
            "If the detector returns spikes (non-empty list), you MUST call ticket_create with "
            "a JSON payload: {\"title\": ..., \"body\": ...}.\n\n"
            "Always include a short explanation and recommended safe remediation steps (non-destructive first).\n\n"
        ),
        tools=[
            bq_query_cost_by_project,
            monitoring_fetch_cpu,
            AgentTool(agent=spike_detector_agent),
            ticket_create
        ]
    )
    print("------------- cloud_cost_agent created -------------")
    return cloud_cost_agent


# --- Analysis orchestration ---
def sequential_analysis(project_id, days, session_id="session-1"):
    # Step A: call the BQ tool (local callable)
    try:
        bq_res = bq_query_cost_by_project(project_id, days)
    except Exception as e:
        print("bq tool error:", e)
        bq_res = []

    # Normalize to DataFrame for local analysis
    try:
        bq_df = pd.DataFrame(bq_res)
    except Exception:
        bq_df = pd.DataFrame([])

    # Step B: find spikes locally from billing_df (the generated CSV)

    try:
        # Ensure datetime
        billing_df["usage_start_time"] = pd.to_datetime(billing_df["usage_start_time"])

        # If tz-aware → convert to naive
        if billing_df["usage_start_time"].dt.tz is not None:
            billing_df["usage_start_time"] = billing_df["usage_start_time"].dt.tz_convert(None)
        # If tz-naive → do nothing

        # Create a cutoff timestamp (naive)
        cutoff = pd.Timestamp.utcnow().replace(tzinfo=None) - pd.Timedelta(days=days)

        # Filter last N days
        proj_df = billing_df[
            (billing_df.project_id == project_id) &
            (billing_df.usage_start_time >= cutoff)
            ].copy()
    except Exception:
        # if billing_df not available or conversion fails, fall back to empty
        proj_df = bq_df.copy()

    if proj_df.empty:
        # no data: return a prompt with that info - agent will handle it
        detector_rows = []
    else:
        # Keep minimal fields for detector to save tokens; include usage_start_time & cost and service/instance if available
        detector_rows = proj_df.sort_values("usage_start_time").tail(days).loc[:, ["usage_start_time", "cost",
                                                                                   "service"]].to_dict(orient="records")
        # Convert timestamps to ISO strings for JSON safety
        for r in detector_rows:
            if isinstance(r.get("usage_start_time"), (pd.Timestamp,)):
                r["usage_start_time"] = str(r["usage_start_time"])

    # Step C: fetch monitoring metrics via local tool
    try:
        mon_res = monitoring_fetch_cpu(days)
    except Exception as e:
        print("monitoring tool error:", e)
        mon_res = []

    # Build a compact prompt that instructs the parent agent to use the embedded spike_detector_tool and ticket_create
    # NOTE: We include detector_rows as a JSON string so the model sees a machine-readable payload.
    prompt_payload = {
        "project_id": project_id,
        "days": days,
        "billing_rows_for_detector": detector_rows,
        "recent_metrics_sample": mon_res
    }

    # Plot the payload to identify the spikes
    # plot_cost_and_metrics(prompt_payload)

    prompt = (
            "You are the Cloud Cost Agent (cloud_cost_agent).\n\n"
            "You will be given a compact JSON payload in 'billing_rows_for_detector'.\n"
            "TASK (strict):\n"
            "  1) CALL the tool named 'spike_detector_tool' with payload {\"rows\": billing_rows_for_detector}.\n"
            "     - The spike_detector_tool will return JSON: {\"spikes\": [...], \"reason\": \"...\"}.\n"
            "  2) If the detector returns spikes (non-empty list), CALL the tool named 'ticket_create' with a JSON payload:\n"
            "       {\"title\": \"Cost Spike detected for <project_id>\",\n"
            "        \"body\": \"<short description of spikes and evidence (include usage_start_time & cost)>\"}\n"
            "  3) Regardless of ticket creation, explain the most likely cause of the cost spike. \n\n"
            "Then provide a prioritized list of safe remediation steps (non-destructive first) in a valid JSON format. \n\n"
            "Also include one suggested follow-up action that requires human approval in a valid JSON format.\n\n"
            "If the ticket is created, include all the ticket details.\n\n"
            "CONSTRAINTS:\n"
            "  - Use only the fields in billing_rows_for_detector for detection (do not invent additional rows).\n"
            "  - OUTPUT final JSON.\n\n"
            "Payload:\n" + json.dumps(prompt_payload)
    )

    # Return the prompt string for your runner to execute with cloud_cost_agent
    return prompt


def extract_final_agent_result(agent_result_json: dict):
    """
    Extracts the final LLM output (natural text or JSON) from the agent_result event list.
    """
    events = agent_result_json.get("agent_result", [])
    final_text_blocks = []

    for event in events:
        content = event.get("content") or {}
        parts = content.get("parts") or []

        for part in parts:
            text = part.get("text")
            if text and text.strip():
                final_text_blocks.append(text)

    if not final_text_blocks:
        return None

    # The last meaningful block is the final agent output
    final_output = final_text_blocks[-1].strip()

    # If wrapped in ```json ... ```, extract inner JSON
    if final_output.startswith("```"):
        import re
        match = re.search(r"```json\s*(.*?)\s*```", final_output, re.DOTALL)
        if match:
            return match.group(1).strip()

    response_json = {"result": final_output}
    return response_json

# --- Async runner invocation ---
async def run_analysis_with_agent(project_id: str, days: int = 30):
    load_or_generate_data(True)

    agent = build_cloud_cost_agent()
    runner = InMemoryRunner(agent=agent)

    prompt = sequential_analysis(project_id=project_id, days=days)
    if prompt is None:
        print("No billing data — nothing to analyze.")
        return

    # pass prompt string to runner
    try:
        # run_debug returns detailed output for debugging
        agent_result = await runner.run_debug(prompt)
        # final_res = extract_final_agent_result(agent_result)
        return {"agent_result": agent_result}
    except Exception as e:
        print("Error running agent:", repr(e))
