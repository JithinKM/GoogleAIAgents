"""
Main script that loads synthetic data (generated by data_generator.py),
defines local tools, constructs the LlmAgent and InMemoryRunner, and runs a sample analysis.
"""
import json
import logging
import pandas as pd
from google.adk.runners import Runner
from google.adk.sessions import InMemorySessionService
from google.adk.plugins.logging_plugin import LoggingPlugin

import data_loader
import tools
import agents

# Configure logging
logger = logging.getLogger(__name__)

# --- Analysis orchestration ---
def sequential_analysis(project_id, days, session_id="session-1"):
    # Step A: call the BQ tool (local callable)
    try:
        bq_res = tools.bq_query_cost_by_project(project_id, days)
    except Exception as e:
        logger.error(f"bq tool error: {e}")
        bq_res = []

    # Normalize to DataFrame for local analysis
    try:
        bq_df = pd.DataFrame(bq_res)
    except Exception:
        bq_df = pd.DataFrame([])

    # Step B: find spikes locally from billing_df (the generated CSV)
    try:
        # Access billing_df from data_loader
        billing_df = data_loader.billing_df.copy()
        
        # Ensure datetime
        billing_df["usage_start_time"] = pd.to_datetime(billing_df["usage_start_time"])

        # If tz-aware → convert to naive
        if billing_df["usage_start_time"].dt.tz is not None:
            billing_df["usage_start_time"] = billing_df["usage_start_time"].dt.tz_convert(None)
        # If tz-naive → do nothing

        # Create a cutoff timestamp (naive)
        cutoff = pd.Timestamp.utcnow().replace(tzinfo=None) - pd.Timedelta(days=days)

        # Filter last N days
        proj_df = billing_df[
            (billing_df.project_id == project_id) &
            (billing_df.usage_start_time >= cutoff)
            ].copy()
    except Exception as e:
        logger.error(f"Error filtering billing data: {e}")
        # if billing_df not available or conversion fails, fall back to empty
        proj_df = bq_df.copy()

    if proj_df.empty:
        # no data: return a prompt with that info - agent will handle it
        detector_rows = []
    else:
        # Keep minimal fields for detector to save tokens; include usage_start_time & cost and service/instance if available
        detector_rows = proj_df.sort_values("usage_start_time").tail(days).loc[:, ["usage_start_time", "cost",
                                                                                   "service"]].to_dict(orient="records")
        # Convert timestamps to ISO strings for JSON safety
        for r in detector_rows:
            if isinstance(r.get("usage_start_time"), (pd.Timestamp,)):
                r["usage_start_time"] = str(r["usage_start_time"])

    # Step C: fetch monitoring metrics via local tool
    try:
        mon_res = tools.monitoring_fetch_cpu(days)
    except Exception as e:
        logger.error(f"monitoring tool error: {e}")
        mon_res = []

    # Build a compact prompt that instructs the parent agent to use the embedded spike_detector_tool and ticket_create
    # NOTE: We include detector_rows as a JSON string so the model sees a machine-readable payload.
    prompt_payload = {
        "project_id": project_id,
        "days": days,
        "billing_rows_for_detector": detector_rows,
        "recent_metrics_sample": mon_res
    }

    prompt = (
            "You are the Cloud Cost Agent (cloud_cost_agent).\n\n"
            "You will be given a compact JSON payload in 'billing_rows_for_detector'.\n"
            "TASK (strict):\n"
            "  1) CALL the tool named 'spike_detector_tool' with payload {\"rows\": billing_rows_for_detector}.\n"
            "     - The spike_detector_tool will return JSON: {\"spikes\": [...], \"reason\": \"...\"}.\n"
            "  2) If the detector returns spikes (non-empty list), CALL the tool named 'ticket_create' with a JSON payload:\n"
            "       {\"title\": \"Cost Spike detected for <project_id>\",\n"
            "        \"body\": \"<short description of spikes and evidence (include usage_start_time & cost)>\"}\n"
            "  3) Regardless of ticket creation, explain the most likely cause of the cost spike. \n\n"
            "Then provide a prioritized list of safe remediation steps (non-destructive first). \n\n"
            "Also include one suggested follow-up action that requires human approval.\n\n"
            "If the ticket is created, include all the ticket details.\n\n"
            "CONSTRAINTS:\n"
            "  - Use only the fields in billing_rows_for_detector for detection (do not invent additional rows).\n"
            "Payload:\n" + json.dumps(prompt_payload)
    )

    # Return the prompt string for your runner to execute with cloud_cost_agent
    return prompt

runner = None
# --- Async runner invocation ---
async def run_analysis_with_agent(project_id: str, days: int = 30):
    global runner
    data_loader.load_or_generate_data(True)

    session_service = InMemorySessionService()
    agent = agents.build_cloud_cost_agent()
    if runner is None:
        runner = Runner(
            agent=agent,
            app_name="Cloud Cost Anomaly Detection App",
            session_service=session_service,
            plugins=[
                LoggingPlugin()
            ]
        )

    prompt = sequential_analysis(project_id=project_id, days=days)
    if prompt is None:
        logger.warning("No billing data — nothing to analyze.")
        return

    # pass prompt string to runner
    try:
        # run_debug returns detailed output for debugging
        agent_result = await runner.run_debug(prompt)
        return {"agent_result": agent_result}
    except Exception as e:
        logger.error(f"Error running agent: {repr(e)}")
